# -*- coding: utf-8 -*-
"""HAR_SmartPhone_CNN+BiLSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JUSDJ72gMwe-fachM0cE9fB7IumalfMB
"""

from google.colab import drive
drive.mount('/content/drive')

!pip3 install optuna==1.5.0
!pip3 install optkeras==0.0.7

import os
import statistics
import csv
import itertools
import tensorflow as tf
import keras

import pandas as pd
import numpy as np
import datetime as dt
import matplotlib.pyplot as plt
import seaborn as sn

from tqdm import tqdm_notebook as tqdm

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import *
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
from keras.utils.vis_utils import plot_model
from tensorflow.keras.models import Model

import optuna
print('Optuna', optuna.__version__)

from optkeras.optkeras import OptKeras
import optkeras
print('OptKeras', optkeras.__version__)
from numpy import mean
from numpy import std
import tensorflow.keras.backend as K
from tensorflow.keras.callbacks import Callback, CSVLogger, ModelCheckpoint

# (Optional) Disable messages from Optuna below WARN level.
optuna.logging.set_verbosity(optuna.logging.WARN)

#cd /saurabh/My Drive/Datasets/har/wisdm-dataset/
data_dir = '/content/drive/MyDrive/wisdm-dataset/wisdm-dataset'
#data_dir = '/content/drive/MyDrive/new_dataset.npz'

ls

# watch = pd.read_pickle('watch.df')
#watch = pd.read_pickle('/content/drive/MyDrive/wisdm-dataset/watch.df')
#phone = pd.read_pickle('/content/drive/MyDrive/wisdm-dataset/wisdm-dataset/phone.df')
phone = pd.read_pickle('/content/drive/MyDrive/phone.df')
#watch = pd.read_pickle('/content/drive/MyDrive/new_dataset.npz')

#watch.shape
phone.shape

def activity_mapper(activity):
    if(activity in ['A', 'B', 'C', 'D', 'E']):
        return 0
    #elif(activity in ['M''P', 'O', 'F', 'Q', 'R','G', 'S']):
        #return 1
    else:(activity in ['M','P', 'O', 'F', 'Q', 'R','G', 'S'])
    return 1
#watch['activity'] = watch['activity'].apply(activity_mapper)
phone['activity'] = phone['activity'].apply(activity_mapper)

#watch.activity.value_counts()
phone.activity.value_counts()

activity_map = {
    0: 'General Non Hand Oriented',
    1: 'General Hand Oriented',
    #2: 'Eating Activity'
}

activities = sorted(activity_map.keys())
activity_encoding = {v: k for k, v in enumerate(activities)}

window_size = 200
stride = 50
frames = []
for i in tqdm(range(0, len(phone)-window_size, stride)):
    window = phone.iloc[i:i+window_size]
    if window['activity'].nunique() == 1:
      frames.append(window)

#activities = sorted(act_map.keys())
#activity_encoding = {v: k for k, v in enumerate(activities)}

X_list = []
y_list = []

#for each frame replace label with activity
for frame in tqdm(frames):
    #X_list.append(frame[['watch_accel_x', 'watch_accel_y', 'watch_accel_z', 'watch_gyro_x', 'watch_gyro_y', 'watch_gyro_z']].values)
    X_list.append(frame[['phone_accel_x', 'phone_accel_y', 'phone_accel_z', 'phone_gyro_x', 'phone_gyro_y', 'phone_gyro_z']].values)
    y_list.append(activity_encoding[frame.iloc[0]['activity']])
    #y_list.append(frame.iloc[0][['activity']].values)

X = np.array(X_list)
y = np.array(to_categorical(y_list))

print(X.shape)
print(y.shape)

print(X.shape[1:])

#data split: as informed by slides split for 30,000 examples
#our example is a window and coming from a big expansive amount of data
#5 second split based on papers using 5 sec split. We can play with this

X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, test_size=0.5, random_state=42)

#train
print("X_train: ",X_train.shape)

#dev
print("X_val: ",X_val.shape)

#test
print("X_test: ",X_test.shape)

print("Y_train: ",y_train.shape)
print("Y_val: ",y_val.shape)
print("Y_test: ",y_test.shape)

# reshape data into time steps of sub-sequences
n_steps, n_length,n_features = 4,50,6
trainX = X_train.reshape((X_train.shape[0], n_steps, n_length, n_features))
valX = X_val.reshape((X_val.shape[0], n_steps, n_length, n_features))
testX = X_test.reshape((X_test.shape[0], n_steps, n_length, n_features))

from keras.models import Sequential
import keras
from keras.layers import LSTM, Conv1D, TimeDistributed, MaxPooling1D
from keras.layers.core import Dense, Dropout, Flatten
#from keras.layers.normalization import BatchNormalization
from tensorflow.keras.layers import BatchNormalization

#timesteps = 128
#input_dim = 9
#n_classes = 6

study_name = 'HAR' + 'Train1'

""" Step 1. Instantiate OptKeras class
You can specify arguments for Optuna's create_study method and other arguments
for OptKeras such as enable_pruning.
"""

ok = OptKeras(study_name=study_name,
              monitor='val_acc',
              direction='maximize')


""" Step 2. Define objective function for Optuna """

def objective(trial):

    """ Step 2.1. Define parameters to try using methods of optuna.trial such as
    suggest_categorical. In this simple demo, try 2*2*2*2 = 16 parameter sets:
    2 values specified in list for each of 4 parameters
    (filters, kernel_size, strides, and activation for convolution).
    """


    model = Sequential()
    input  = Input(shape=(None,n_length,n_features),name='base_input')
    td_conv_1 = TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='gelu'))(input)
    td_maxp_1 = TimeDistributed(MaxPooling1D(pool_size=2))(td_conv_1)
    td_conv_2 = TimeDistributed(Conv1D(filters=64, kernel_size=5, activation='gelu'))(td_maxp_1)
    td_maxp_2 = TimeDistributed(MaxPooling1D(pool_size=2))(td_conv_2)
    td_conv_3 = TimeDistributed(Conv1D(filters=32, kernel_size=7, activation='gelu'))(td_maxp_2)
    td_drop_1 = TimeDistributed(Dropout(0.06))(td_conv_3)
    td_maxp_3 = TimeDistributed(MaxPooling1D(pool_size=2))(td_drop_1)
    td_flat_1 = TimeDistributed(Flatten())(td_maxp_3)
    bd_lstm_1 = Bidirectional(LSTM(128))(td_flat_1)
    bd_lstm_2 = Bidirectional(LSTM(128))(td_flat_1)
    concat  = concatenate([bd_lstm_1 , bd_lstm_2] , name="concatenation")
    dense_1 = Dense(128,activation='gelu')(concat)
    ln = LayerNormalization(axis=-1 , center=True , scale=True)(dense_1)
    output = Dense(y_train.shape[1], activation = 'softmax')(ln)
    model = Model(inputs=input,outputs=output)
   # model.add(TimeDistributed(Conv1D(filters=trial.suggest_categorical('filters1', [32,64,96]), kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))
   # model.add(TimeDistributed(MaxPooling1D(pool_size=2)))
   # model.add(TimeDistributed(Conv1D(filters=trial.suggest_categorical('filters2', [32,64,96]), kernel_size=3, activation='relu')))
   # model.add(TimeDistributed(MaxPooling1D(pool_size=2)))
   # model.add(TimeDistributed(Dropout(rate = trial.suggest_categorical('drop1', [0.05,0.06,0.07]))))
   # model.add(TimeDistributed(Flatten()))
  # model.add(LSTM(128 , return_sequences = True))
   # model.add(LSTM(128))
   # model.add(Dropout(rate = trial.suggest_categorical('drop2', [0.1,0.2,0.3])))
  #  model.add(Dense(y_train.shape[1], activation = 'softmax'))
   # model.summary()
    """ Step 2.2. Specify callbacks(trial) and keras_verbose in fit
    (or fit_generator) method of Keras model
    """
    model.compile(Adam(),loss=tf.keras.losses.categorical_crossentropy,metrics=['acc'])
    model.fit(trainX,y_train,
              validation_data=(valX, y_val), shuffle=True,
              batch_size=128, epochs=10,
              #callbacks=ok.callbacks(trial),
              verbose=ok.keras_verbose )


    """ Step 2.3. Return trial_best_value (or latest_value) """
    return ok.trial_best_value

""" Step 3. Run optimize.
Set n_trials and/or timeout (in sec) for optimization by Optuna
"""
ok.optimize(objective, timeout = 90)

print('Best trial number: ', ok.best_trial.number)
print('Best value:', ok.best_trial.value)
print('Best parameters: \n', ok.best_trial.params)

print("Best parameters (retrieved directly from Optuna)", ok.study.best_trial.params)

""" Check the Optuna CSV log file """
pd.options.display.max_rows = 8 # limit rows to display
print('Data Frame read from', ok.optuna_log_file_path, '\n')
display(pd.read_csv(ok.optuna_log_file_path))

""" Check the Keras CSV log file """
pd.options.display.max_rows = 8 # limit rows to display
print('Data Frame read from', ok.keras_log_file_path, '\n')
display(pd.read_csv(ok.keras_log_file_path))

# define model
model = Sequential()
input  = Input(shape=(None,n_length,n_features),name='base_input')
td_conv_1 = TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='gelu'))(input)
td_maxp_1 = TimeDistributed(MaxPooling1D(pool_size=2))(td_conv_1)
td_conv_2 = TimeDistributed(Conv1D(filters=64, kernel_size=5, activation='gelu'))(td_maxp_1)
td_maxp_2 = TimeDistributed(MaxPooling1D(pool_size=2))(td_conv_2)
td_conv_3 = TimeDistributed(Conv1D(filters=32, kernel_size=7, activation='gelu'))(td_maxp_2)
td_drop_1 = TimeDistributed(Dropout(0.06))(td_conv_3)
td_maxp_3 = TimeDistributed(MaxPooling1D(pool_size=2))(td_drop_1)
td_flat_1 = TimeDistributed(Flatten())(td_maxp_3)
bd_lstm_1 = Bidirectional(LSTM(128))(td_flat_1)
bd_lstm_2 = Bidirectional(LSTM(128))(td_flat_1)
concat  = concatenate([bd_lstm_1 , bd_lstm_2] , name="concatenation")
dense_1 = Dense(128,activation='gelu')(concat)
ln = LayerNormalization(axis=-1 , center=True , scale=True)(dense_1)
output = Dense(y_train.shape[1], activation = 'softmax')(ln)
model = Model(inputs=input,outputs=output)
#model.add(TimeDistributed(Conv1D(filters=96, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))
#model.add(TimeDistributed(MaxPooling1D(pool_size=2)))
#model.add(TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='relu')))
#model.add(TimeDistributed(MaxPooling1D(pool_size=2)))
#model.add(TimeDistributed(Dropout(0.06)))
#model.add(TimeDistributed(Flatten()))
#model.add(LSTM(64 , return_sequences= True ))
#model.add(LSTM(64))
#model.add(Dropout(0.1))
#model.add(Dense(y_train.shape[1], activation = 'softmax'))

plot_model(model, to_file='model_plot_cnnbilstm.png', show_shapes=True,rankdir = 'TB',expand_nested = False,  show_layer_names=False, show_dtype =False)

y_train.shape

model.compile(Adam(),
              loss=tf.keras.losses.categorical_crossentropy,
              metrics=['acc'])

from tensorflow.keras.callbacks import ModelCheckpoint

# Define the checkpoint callback
checkpoint_callback = ModelCheckpoint(
    filepath='model_checkpoint.h5',
    monitor='val_loss',  # Monitor validation loss to save the best model
    save_best_only=True,  # Save only the best model
    save_weights_only=False,  # Save entire model, not just weights
    verbose=1
)

from tensorflow.keras.callbacks import EarlyStopping

# Define the early stopping callback
early_stopping_callback = EarlyStopping(
    monitor='val_loss',  # Monitor validation loss to decide when to stop
    patience=5,  # Number of epochs with no improvement after which training will stop
    verbose=1,
    restore_best_weights=True  # Restore the best model weights at the end of training
)

from tensorflow.keras.callbacks import ReduceLROnPlateau

# Define the reduce learning rate on plateau callback
reduce_lr_callback = ReduceLROnPlateau(
    monitor='val_loss',  # Monitor validation loss to decide when to reduce LR
    factor=0.1,  # Factor by which the learning rate will be reduced
    patience=3,  # Number of epochs with no improvement after which LR will be reduced
    verbose=1
)

from tensorflow.keras.callbacks import Callback

class PrintLearningRate(Callback):
    def on_epoch_begin(self, epoch, logs=None):
        lr = self.model.optimizer.lr.numpy()
        print(f"Learning rate for epoch {epoch+1}: {lr}")

# Usage:
print_lr_callback = PrintLearningRate()

from tensorflow.keras.callbacks import TensorBoard

# Define the TensorBoard callback
tensorboard_callback = TensorBoard(
    log_dir='./logs',
    histogram_freq=1,  # How often to compute histograms for layers' activations
    write_graph=True,  # Whether to write the model graph to TensorBoard
    write_images=True,  # Whether to write images of model layers to TensorBoard
)

history = model.fit(trainX,y_train,callbacks=[checkpoint_callback, early_stopping_callback, reduce_lr_callback, print_lr_callback, tensorboard_callback],epochs=25, validation_data=(valX, y_val), verbose=1 , batch_size  = 64 )

# Reverse activity encoding encodes index like 3 -> eating sandwich
reverse_activity_encoding = {v: activity_map[k] for k, v in activity_encoding.items()}
lstm_y_val_pred = model.predict(valX)

y_val_argmax = y_val.argmax(axis=1)
y_val_pred_argmax = lstm_y_val_pred.argmax(axis=1)

lstm_actual_val_activities = [reverse_activity_encoding[i] for i in y_val_argmax]
lstm_predicted_val_activities = [reverse_activity_encoding[i] for i in y_val_pred_argmax]

##### Below is Final Test Set results ######
lstm_y_test_pred = model.predict(testX)
lstm_y_test_argmax = y_test.argmax(axis=1)

lstm_y_test_pred_argmax = lstm_y_test_pred.argmax(axis=1)

lstm_actual_test_activities = [reverse_activity_encoding[i] for i in lstm_y_test_argmax]
lstm_predicted_test_activities = [reverse_activity_encoding[i] for i in lstm_y_test_pred_argmax]

from sklearn.metrics import classification_report
#print("             SmartWatch Results with CNN-BiLSTM for Validation Set\n\n" + classification_report(lstm_predicted_val_activities,lstm_actual_val_activities , digits = 4))
#print("             SmartWatch Results with CNN-BiLSTM for Test Set\n\n" + classification_report(lstm_predicted_test_activities,lstm_actual_test_activities , digits = 4))

print("             Smart Phone Results with CNN-BiLSTM for Validation Set\n\n" + classification_report(lstm_predicted_val_activities,lstm_actual_val_activities , digits = 4))
print("             Smart Phone Results with CNN-BiLSTM for Test Set\n\n" + classification_report(lstm_predicted_test_activities,lstm_actual_test_activities , digits = 4))

import seaborn as sns
from sklearn.metrics import confusion_matrix
lstm_cm = confusion_matrix(lstm_y_test_argmax, lstm_y_test_pred_argmax)
df_cm = pd.DataFrame(lstm_cm, index = [reverse_activity_encoding[i] for i in range(2)], columns = [reverse_activity_encoding[i] for i in range(2)])
#plt.figure(figsize = (10,10))
#plt.title('Smartwatch CNN-BiLSTM Confusion Matrix')
plt.title('Smart Phone CNN-BiLSTM Confusion Matrix')
sns.heatmap(df_cm, annot=True, fmt='g')

metrics_history = pd.DataFrame(history.history)
metrics_history[['loss', 'val_loss' , 'acc' , 'val_acc']].plot(title = 'Model Accuracy & Loss CNN-BiLSTM' , xlabel = 'epoch' , ylabel = 'Accuracy & Loss') #loss = train loss, val_loss = validation Loss

def evaluate_model(trainX, y_train, testX, y_test):
	# define model
# reshape data into time steps of sub-sequences
  n_steps, n_length,n_features = 4,50,6
  trainX = X_train.reshape((X_train.shape[0], n_steps, n_length, n_features))
  valX = X_val.reshape((X_val.shape[0], n_steps, n_length, n_features))
  testX = X_test.reshape((X_test.shape[0], n_steps, n_length, n_features))
# define model
# define model
  model = Sequential()
  input  = Input(shape=(None,n_length,n_features),name='base_input')
  td_conv_1 = TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='gelu'))(input)
  td_maxp_1 = TimeDistributed(MaxPooling1D(pool_size=2))(td_conv_1)
  td_conv_2 = TimeDistributed(Conv1D(filters=64, kernel_size=5, activation='gelu'))(td_maxp_1)
  td_maxp_2 = TimeDistributed(MaxPooling1D(pool_size=2))(td_conv_2)
  td_conv_3 = TimeDistributed(Conv1D(filters=32, kernel_size=7, activation='gelu'))(td_maxp_2)
  td_drop_1 = TimeDistributed(Dropout(0.06))(td_conv_3)
  td_maxp_3 = TimeDistributed(MaxPooling1D(pool_size=2))(td_drop_1)
  td_flat_1 = TimeDistributed(Flatten())(td_maxp_3)
  bd_lstm_1 = Bidirectional(LSTM(32))(td_flat_1)
  bd_lstm_2 = Bidirectional(LSTM(64))(td_flat_1)
  concat  = concatenate([bd_lstm_1 , bd_lstm_2] , name="concatenation")
  dense_1 = Dense(128,activation='gelu')(concat)
  ln = LayerNormalization(axis=-1 , center=True , scale=True)(dense_1)
  output = Dense(y_train.shape[1], activation = 'softmax')(ln)
  model = Model(inputs=input,outputs=output)
  #model.add(TimeDistributed(Conv1D(filters=96, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))
  #model.add(TimeDistributed(MaxPooling1D(pool_size=2)))
  #model.add(TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='relu')))
  #model.add(TimeDistributed(MaxPooling1D(pool_size=2)))
  #model.add(TimeDistributed(Dropout(0.06)))
  #model.add(TimeDistributed(Flatten()))
  #model.add(LSTM(64 , return_sequences= True ))
  #model.add(LSTM(64))
  #model.add(Dropout(0.1))
  #model.add(Dense(y_train.shape[1], activation = 'softmax'))
  model.compile(Adam(),
              loss=tf.keras.losses.categorical_crossentropy,
              metrics=['acc'])
# fit network
  model.fit(trainX, y_train, epochs=25, batch_size=64, validation_data=(valX, y_val),verbose=0)
# evaluate model
  _, accuracy = model.evaluate(testX, y_test, batch_size=64, verbose=0)
  return accuracy

# summarize scores
def summarize_results(scores):
	print(scores)
	m, s = mean(scores), std(scores)
	print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))

# run an experiment
def run_experiment(repeats=10):
	# load data
	#trainX, trainy, testX, testy = load_dataset()
	# repeat experiment
	scores = list()
	for r in range(repeats):
		score = evaluate_model(trainX, y_train, testX, y_test)
		score = score * 100.0
		print('>#%d: %.3f' % (r+1, score))
		scores.append(score)
	# summarize results
	summarize_results(scores)

run_experiment()